{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\germa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!spacy download es_core_news_sm\n",
    "\n",
    "    # Cargar modelo preentrenado para español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "sentence = \"Creo que todos pensamos y podemos decir que la remuneración no es la correcta. Si bien nos alcanza, está por debajo del valor de mercado.\"\n",
    "\n",
    "sentence = \"La semana pasada la comunicación del equipo fue inefectiva.\"\n",
    "# Procesar la oración\n",
    "doc = nlp(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('La', 'NNP'),\n",
       " ('semana', 'VBD'),\n",
       " ('pasada', 'JJ'),\n",
       " ('la', 'NN'),\n",
       " ('comunicación', 'NN'),\n",
       " ('del', 'NN'),\n",
       " ('equipo', 'NN'),\n",
       " ('fue', 'NN'),\n",
       " ('inefectiva', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Etiquetado POS (Part of Speech)\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   84. ADJ  ,     2\n",
      "   85. ADP  ,     1\n",
      "   87. AUX  ,     1\n",
      "   90. DET  ,     2\n",
      "   92. NOUN ,     3\n",
      "   97. PUNCT,     1\n"
     ]
    }
   ],
   "source": [
    "POS_count = doc.count_by(spacy.attrs.POS)\n",
    "\n",
    "for k, v in sorted(POS_count.items()):\n",
    "    print(f\"{k:{5}}. {doc.vocab[k].text:{5}}, {v:{5}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La         DET                  determiner           \n",
      "semana     NOUN                 noun                 \n",
      "pasada     ADJ                  adjective            \n",
      "la         DET                  determiner           \n",
      "comunicación NOUN                 noun                 \n",
      "del        ADP                  adposition           \n",
      "equipo     NOUN                 noun                 \n",
      "fue        AUX                  auxiliary            \n",
      "inefectiva ADJ                  adjective            \n",
      ".          PUNCT                punctuation          \n",
      "['semana', 'comunicación', 'equipo']\n"
     ]
    }
   ],
   "source": [
    "nouns = []\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{20}} {spacy.explain(token.pos_):{20}} ')\n",
    "    \n",
    "    if(token.pos_ == 'NOUN'):\n",
    "        nouns.append(token.text)\n",
    "        \n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras de interés: ['semana', 'comunicación', 'equipo']\n",
      "La palabra más relevante es: equipo\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo de lenguaje preentrenado en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Oración de interés\n",
    "#sentence = \"Creo que todos pensamos y podemos decir que la remuneración no es la correcta. Si bien nos alcanza, está por debajo del valor de mercado.\"\n",
    "\n",
    "# Tokenizar la oración\n",
    "tokens = nlp(sentence)\n",
    "\n",
    "# Palabras de interés\n",
    "palabras_interes = [token.text for token in tokens if token.pos_ == 'NOUN']\n",
    "\n",
    "print(\"Palabras de interés:\", palabras_interes)\n",
    "\n",
    "# Obtener los vectores de embedding para todas las palabras en la oración\n",
    "vectores = {token.text: token.vector for token in tokens}\n",
    "\n",
    "# Calcular la similitud coseno entre los vectores de las palabras de interés y todas las palabras en la oración\n",
    "similitudes = {palabra: np.dot(vectores[palabra], vectores_interes) / (np.linalg.norm(vectores[palabra]) * np.linalg.norm(vectores_interes)) for palabra in vectores for vectores_interes in [vectores[p] for p in palabras_interes]}\n",
    "\n",
    "# Encontrar la palabra más relevante basándose en la similitud\n",
    "palabra_relevante = max(similitudes, key=similitudes.get)\n",
    "\n",
    "print(f\"La palabra más relevante es: {palabra_relevante}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras de interés: ['semana', 'comunicación', 'equipo']\n",
      "La palabra más relevante es: equipo\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el modelo de lenguaje preentrenado en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Oración de interés\n",
    "#sentence = \"Creo que todos pensamos y podemos decir que la remuneración no es la correcta. Si bien nos alcanza, está por debajo del valor de mercado.\"\n",
    "\n",
    "# Tokenizar la oración\n",
    "tokens = nlp(sentence)\n",
    "\n",
    "# Palabras de interés\n",
    "palabras_interes = [token.text for token in tokens if token.pos_ == 'NOUN']\n",
    "\n",
    "print(\"Palabras de interés:\", palabras_interes)\n",
    "\n",
    "# Obtener los vectores de embedding solo para sustantivos\n",
    "vectores = {token.text: token.vector for token in tokens if token.pos_ == 'NOUN'}\n",
    "\n",
    "# Calcular la similitud coseno entre los vectores de las palabras de interés y todos los sustantivos en la oración\n",
    "similitudes = {palabra: np.dot(vectores[palabra], vectores_interes) / (np.linalg.norm(vectores[palabra]) * np.linalg.norm(vectores_interes)) for palabra in vectores for vectores_interes in [vectores[p] for p in palabras_interes]}\n",
    "\n",
    "# Encontrar la palabra más relevante basándose en la similitud\n",
    "palabra_relevante = max(similitudes, key=similitudes.get)\n",
    "\n",
    "print(f\"La palabra más relevante es: {palabra_relevante}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.083*\"pensamos\" + 0.083*\"decir\" + 0.083*\"si\" + 0.083*\"remuneración\" + 0.083*\"podemos\"')\n"
     ]
    }
   ],
   "source": [
    "# Instala gensim si no lo has hecho ya: pip install gensim\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Descarga las stopwords de NLTK si no lo has hecho ya:\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Definir la lista de comentarios\n",
    "comentarios = [ \"Creo que todos pensamos y podemos decir que la remuneración no es la correcta. Si bien nos alcanza, está por debajo del valor de mercado.\"]\n",
    "\n",
    "# Tokenización y preprocesamiento de texto\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('spanish'))  # Puedes cambiar a 'english' si trabajas en inglés\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Crear un diccionario y un corpus\n",
    "tokenized_comments = [preprocess_text(comment) for comment in comentarios]\n",
    "dictionary = corpora.Dictionary(tokenized_comments)\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_comments]\n",
    "\n",
    "# Entrenar el modelo LDA\n",
    "lda_model = LdaModel(corpus, num_topics=1, id2word=dictionary, passes=100)\n",
    "\n",
    "# Imprimir los tópicos\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
